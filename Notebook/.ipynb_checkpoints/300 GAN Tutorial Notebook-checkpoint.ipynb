{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300 GAN Tutorial Notebook\n",
    "\n",
    "This Notebook should give a quick introduction to the inner workings of GANs by showing how to write a GAN that produces handwritten digits from the MNIST Database. The code is dependant on keras, matplotlib and numpy while using Tensorflow 2.X.\n",
    "\n",
    "\n",
    "## Sources:\n",
    "MNIST Database:\n",
    " - http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Papers:\n",
    " - https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\n",
    "\n",
    "Tutorials:\n",
    " - https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb#scrollTo=gL5byGhNzOzd\n",
    " - https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    " \n",
    "\n",
    "## Training a GAN\n",
    "\n",
    "### Step 1\n",
    "\n",
    "Training takes place in two steps shown below. In the first step, the discriminator is trained to classify between real and generated digits. A vector from the latent space (*LATENT_DIM*) full of random numbers is inputted into the generator. The generator takes these numbers and creates a small image with dimensions 7 x 7. Then, this small image is upscaled with *Conv2DTranspose* to 14 x 14 and then to 28 x 28 in the subsequent two layers. We finally apply sigmoid to the values and have successfully generated an image from a vector from the latent space.\n",
    "\n",
    "128 *image_samples* from the MNIST dataset and 128 *generated_images* form the dataset *images_all* and values *y_all* for the first step. This is passed to the discriminator, while real images are labeled with 1 and generated \"fake\" images are labeled with 0. Backpropagation is applied to optimize the discriminator.\n",
    "\n",
    "\n",
    "### Step 2\n",
    "\n",
    "The second step trains the generator. Generator and discriminator are combined to form one big network called *gan_model*. d_model.trainable = False makes sure the discriminator is not trained. Making a big model helps make the code for training simpler.\n",
    "\n",
    "Again, a full batch (*n_batch = 256*) of vectors from the latent space is generated. They are passed into the *gan_model*, which generates fake images inside it and then uses the discriminator to try and tell real and generated images apart.\n",
    "\n",
    "y_generator_flipped = np.ones((n_batch, 1)), because the generator should be taught to trick the discriminator into thinking that fake generated images (0) are actually real (1). This metric is used to optimize the generator.\n",
    "\n",
    "\n",
    "<img src=\"GAN with MNIST-1.png\">\n",
    "<img src=\"GAN with MNIST-2.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f88f533948>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Expand Dimensions to 3D to add one for the color channel of black and white: expand_dims(train_images, axis=-1)\n",
    "# Normalize the image by dividing by 255 --> faster convergence\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images, test_images = np.expand_dims(train_images, axis=-1) / 255.0, np.expand_dims(train_images, axis=-1) / 255.0\n",
    "\n",
    "\n",
    "# Examples\n",
    "# Shape is (60000, 28, 28, 1) for 60 000 images with dimensions 28 x 28 and 1 color channel (black and white)\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "plt.imshow(train_images[0, :, :, 0], cmap='gray_r')\n",
    "plt.imshow(train_images[42, :, :, 0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99135029  0.20260851 -1.03799632 -0.64160533 -1.63986893  1.18585547\n",
      "  -0.14389768  0.77598984  0.1729118  -0.52700883  0.41263793  1.30791926\n",
      "  -0.45463668  1.96842719  0.09341416 -0.43718706 -0.61900659 -1.16332592\n",
      "  -1.72466484 -0.09300866 -0.26206198 -0.23657669 -0.33997426 -1.35234981\n",
      "  -0.42896121  0.25227193  0.36468466 -0.64470028  0.02674817 -0.01138662\n",
      "  -1.45650008 -0.03154641 -0.62816304 -0.69153291 -0.7708117  -0.18408072\n",
      "   0.04983312 -1.7305672   1.87321838 -0.01841083 -0.74389071  2.46725246\n",
      "   0.06075403 -0.10999764 -0.17987015 -1.3390663  -1.24578983  0.70657427\n",
      "   0.03582405  0.97999196  0.74086061 -0.36567633  0.16163041 -0.97297847\n",
      "  -0.78648493  0.16278246  0.7800854  -1.28244364  1.33590928  0.3094771\n",
      "  -0.56542156 -0.724249    0.38137412 -0.55623255 -1.49709667  0.24002343\n",
      "   0.84265301 -0.70792023  1.79560782  1.16542192 -0.94116912  0.43341626\n",
      "   0.69917027  0.69100149 -1.09001207  0.6755487  -0.44333785  0.11354847\n",
      "   0.11997992 -0.66580042 -0.85247326  1.34013899  0.58852169  0.38434086\n",
      "  -0.65814799 -0.12445938  0.9364653   0.52761322 -0.87228754 -0.85306813\n",
      "  -0.43902798  0.46181017  0.23819475 -0.09584056  0.62605663 -1.13116186\n",
      "  -0.09873202 -0.24587354 -0.63183045  0.90858437]]\n"
     ]
    }
   ],
   "source": [
    "# Two helper functions\n",
    "\n",
    "# Sample minibatch of m MNIST samples from seed:\n",
    "def get_MNIST_samples(n_samples):\n",
    "    # Pick new image samples from dataset\n",
    "    # Pick random images to help with gradient descent\n",
    "    image_samples = np.zeros((n_samples, 28, 28, 1))\n",
    "\n",
    "    for i in range(0, n_samples):\n",
    "        index = random.randint(0, 59999)\n",
    "        image_samples[i] = train_images[index]\n",
    "\n",
    "    # Generate actual class labels (1)\n",
    "    y_MNIST = np.ones((n_samples, 1))\n",
    "\n",
    "    return image_samples, y_MNIST\n",
    "\n",
    "\n",
    "# Sample minibatch of m noise samples from seed:\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    latent_points = np.random.randn(latent_dim * n_samples)\n",
    "\n",
    "    # reshape into a batch of inputs for the network\n",
    "    latent_points = latent_points.reshape(n_samples, latent_dim)\n",
    "\n",
    "    return latent_points\n",
    "\n",
    "\n",
    "# Examples\n",
    "latent_points_example = generate_latent_points(100, 1)\n",
    "print(latent_points_example)\n",
    "\n",
    "# Examples\n",
    "MNIST_example = get_MNIST_samples(1)\n",
    "# plt.imshow(MNIST_example[0, :, :, 0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Output:  [[0.47714648]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define all 3 models with the help of Keras\n",
    "\n",
    "# Define the standalone generator model\n",
    "def build_generator(LATENT_DIM=100):\n",
    "    model = Sequential()\n",
    "\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, input_dim=LATENT_DIM))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "\n",
    "    # Upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7, 7), activation='sigmoid', padding='same'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the standalone discriminator model\n",
    "def build_discriminator(in_shape=(28, 28, 1)):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the combined generator and discriminator model, for updating the generator\n",
    "def build_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "\n",
    "    # compile model\n",
    "    optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Example Generator\n",
    "generator_example = build_generator()\n",
    "latent_points_example = generate_latent_points(100, 1)\n",
    "\n",
    "generated_images_example = generator_example.predict(latent_points_example)\n",
    "plt.imshow(generated_images_example[0, :, :, 0], cmap='gray_r')\n",
    "\n",
    "# Example Discriminator\n",
    "discriminator_example = build_discriminator()\n",
    "\n",
    "prediction_example = discriminator_example.predict(generated_images_example)\n",
    "print('Discriminator Output: ', prediction_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(generator, discriminator, gan_model, LATENT_DIM, n_epochs=100, n_batch=256):\n",
    "    \n",
    "    batches_per_epoch = int(60000 / n_batch)\n",
    "    n_half_batch = int(n_batch / 2)\n",
    "\n",
    "    for epoch in range(0, n_epochs):\n",
    "        for batch in range(0, batches_per_epoch):\n",
    "            \n",
    "            # Get real image samples from MNIST dataset\n",
    "            image_samples, y = get_MNIST_samples(n_half_batch)\n",
    "            # Generate points from the latent space (Size: n_half_batch)\n",
    "            latent_points = generate_latent_points(LATENT_DIM, n_half_batch)\n",
    "            # Generate 'fake' class label (0)\n",
    "            y_generator = np.zeros((n_half_batch, 1))\n",
    "\n",
    "            # Generate fake images by inputting the latent_points into the generator\n",
    "            generated_images = generator.predict(latent_points)\n",
    "\n",
    "            # Step 1 ************\n",
    "            # np.vstack stacks arrays in sequence vertically (row wise) -> combine real and generated datasets\n",
    "            images_all, y_all = np.vstack((image_samples, generated_images)), np.vstack((y, y_generator))\n",
    "            # Update the discriminator loss\n",
    "            discriminator_loss, _ = discriminator.train_on_batch(images_all, y_all)\n",
    "\n",
    "            # Step 2 ************\n",
    "            # Generate points from the latent space (Size: this time n_batch)\n",
    "            latent_points = generate_latent_points(LATENT_DIM, n_batch)\n",
    "            # Generate 'fake' class label, but flip them to trip disciminator into thinking it is real (not 0, but 1)\n",
    "            y_generator_flipped = np.ones((n_batch, 1))\n",
    "            generator_loss = gan_model.train_on_batch(latent_points, y_generator_flipped)\n",
    "\n",
    "            print('Epoch: ', epoch, '   Batch Number: ', batch, ' / ', batches_per_epoch, '   Generator Loss: ', generator_loss, '   Discriminator Loss: ', discriminator_loss)\n",
    "\n",
    "            if batch % 25 == 0:\n",
    "                # Matplotlib to capture generated the images ###\n",
    "                fig = plt.figure()\n",
    "                for a in range(32):\n",
    "                    ax = fig.add_subplot(6, 6, a + 1)\n",
    "                    plt.imshow(generated_images[a, :, :, 0], cmap='gray_r')\n",
    "\n",
    "                # Plots are saved with 4 digit numbers in the name to avoid flickering in the video\n",
    "                plt.savefig('plot  epoch ' + '%02d' % epoch + '  batch ' + '%05d' % batch + '.png', dpi=600)\n",
    "                plt.close()\n",
    "                # End Matplotlib                             ###\n",
    "\n",
    "                # Save the generator and discriminator\n",
    "                generator.save('generator.h5')\n",
    "                discriminator.save('discriminator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  0  /  234    Generator Loss:  0.720980703830719    Discriminator Loss:  0.6956827640533447\n",
      "Epoch:  0    Batch Number:  1  /  234    Generator Loss:  0.7416566610336304    Discriminator Loss:  0.6865015029907227\n",
      "Epoch:  0    Batch Number:  2  /  234    Generator Loss:  0.7594889402389526    Discriminator Loss:  0.676751971244812\n",
      "Epoch:  0    Batch Number:  3  /  234    Generator Loss:  0.7785246968269348    Discriminator Loss:  0.6650537252426147\n",
      "Epoch:  0    Batch Number:  4  /  234    Generator Loss:  0.7952412366867065    Discriminator Loss:  0.6633268594741821\n",
      "Epoch:  0    Batch Number:  5  /  234    Generator Loss:  0.804670512676239    Discriminator Loss:  0.6584080457687378\n",
      "Epoch:  0    Batch Number:  6  /  234    Generator Loss:  0.8220213055610657    Discriminator Loss:  0.6521784067153931\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-063b5ece31a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLATENT_DIM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-a67c4b13da31>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(generator, discriminator, gan_model, LATENT_DIM, n_epochs, n_batch)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;31m# Generate fake images by inputting the latent_points into the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mgenerated_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;31m# Step 1 ************\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m     87\u001b[0m           method.__name__))\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1266\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1268\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1269\u001b[0m             \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m             \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    616\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build the GAN ************\n",
    "\n",
    "# Size of the latent space\n",
    "LATENT_DIM = 100\n",
    "\n",
    "\n",
    "# Define the discriminator\n",
    "discriminator = build_discriminator()\n",
    "# Define the generator\n",
    "generator = build_generator(LATENT_DIM)\n",
    "# Define the gan\n",
    "gan_model = build_gan(generator, discriminator)\n",
    "\n",
    "\n",
    "# Train model\n",
    "train(generator, discriminator, gan_model, LATENT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Notebook",
   "language": "python",
   "name": "notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
