{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300 GAN Tutorial Notebook\n",
    "\n",
    "This Notebook should give a quick introduction to the inner workings of GANs by showing how to write a GAN that produces handwritten digits from the MNIST Database. The code is dependant on keras, matplotlib and numpy while using Tensorflow 2.X.\n",
    "\n",
    "\n",
    "## Sources:\n",
    "MNIST Database:\n",
    " - http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Papers:\n",
    " - https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\n",
    "\n",
    "Tutorials:\n",
    " - https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb#scrollTo=gL5byGhNzOzd\n",
    " - https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    " \n",
    "\n",
    "## Training a GAN\n",
    "\n",
    "### Step 1\n",
    "\n",
    "Training takes place in two steps shown below. In the first step, the discriminator is trained to classify between real and generated digits. A vector from the latent space (*LATENT_DIM*) full of random numbers is inputted into the generator. The generator takes these numbers and creates a small image with dimensions 7 x 7. Then, this small image is upscaled with *Conv2DTranspose* to 14 x 14 and then to 28 x 28 in the subsequent two layers. We finally apply sigmoid to the values and have successfully generated an image from a vector from the latent space.\n",
    "\n",
    "128 *image_samples* from the MNIST dataset and 128 *generated_images* form the dataset *images_all* and values *y_all* for the first step. This is passed to the discriminator, while real images are labeled with 1 and generated \"fake\" images are labeled with 0. Backpropagation is applied to optimize the discriminator.\n",
    "\n",
    "\n",
    "### Step 2\n",
    "\n",
    "The second step trains the generator. Generator and discriminator are combined to form one big network called *gan_model*. d_model.trainable = False makes sure the discriminator is not trained. Making a big model helps make the code for training simpler.\n",
    "\n",
    "Again, a full batch (*n_batch = 256*) of vectors from the latent space is generated. They are passed into the *gan_model*, which generates fake images inside it and then uses the discriminator to try and tell real and generated images apart.\n",
    "\n",
    "y_generator_flipped = np.ones((n_batch, 1)), because the generator should be taught to trick the discriminator into thinking that fake generated images (0) are actually real (1). This metric is used to optimize the generator.\n",
    "\n",
    "\n",
    "<img src=\"GAN with MNIST-1.png\">\n",
    "<img src=\"GAN with MNIST-2.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a557243908>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Expand Dimensions to 3D to add one for the color channel of black and white: expand_dims(train_images, axis=-1)\n",
    "# Normalize the image by dividing by 255 --> faster convergence\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images, test_images = np.expand_dims(train_images, axis=-1) / 255.0, np.expand_dims(train_images, axis=-1) / 255.0\n",
    "\n",
    "\n",
    "# Examples\n",
    "# Shape is (60000, 28, 28, 1) for 60 000 images with dimensions 28 x 28 and 1 color channel (black and white)\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "plt.imshow(train_images[0, :, :, 0], cmap='gray_r')\n",
    "plt.imshow(train_images[42, :, :, 0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.71048738e+00  6.24894818e-01  1.82559324e-01  1.26745636e+00\n",
      "   4.50623836e-01 -1.61293434e-01 -2.73237955e-01  4.84640636e-01\n",
      "  -8.64686765e-01 -1.03208651e+00 -2.15731375e-01 -5.68371617e-01\n",
      "   5.03316961e-01 -1.31112929e+00 -8.46018454e-01 -7.18483950e-01\n",
      "   2.44011301e-01 -1.82653910e-01 -1.05756141e+00  2.33687956e-01\n",
      "   9.41271138e-02  3.95932572e-02  4.78188911e-01  2.37661512e-01\n",
      "  -9.72681093e-01  1.72230552e+00  2.86259364e-01  1.02034630e+00\n",
      "  -2.36900174e+00 -1.65794230e+00  1.07340318e+00  1.24468821e+00\n",
      "  -1.36002891e+00  3.87263790e-01  7.22997447e-01  1.39602157e+00\n",
      "   1.12140831e-01  1.30355261e+00  4.03475324e-01 -2.59927248e-01\n",
      "  -4.73738440e-01  6.99137556e-01 -1.78973376e+00  7.09073884e-01\n",
      "   3.37758718e-02 -1.47255241e-02  1.57133315e-01 -2.96967049e-01\n",
      "   1.49776670e+00  9.36882027e-01  1.50345013e+00 -2.02815438e+00\n",
      "   2.50558047e-02  3.72794511e-01 -3.17160019e-01  9.92228316e-02\n",
      "   7.80307802e-01 -1.28425852e+00  2.09980778e-01  8.66979511e-01\n",
      "   1.49415151e-01  2.24006693e-01  3.52715110e+00  1.98228260e-01\n",
      "   9.77731817e-01 -1.56831794e+00 -1.00647062e+00  1.23902748e+00\n",
      "   1.43379789e-01  4.85378082e-01  1.69293235e+00 -1.26305141e+00\n",
      "  -3.11167621e-01 -2.90262536e+00 -1.32289609e-03 -2.57703583e-01\n",
      "  -4.48614675e-01 -1.32207915e-01  1.23665081e+00  1.40544838e+00\n",
      "   5.78899450e-01 -9.55287101e-02  1.43804730e+00  1.83361962e+00\n",
      "  -6.83413528e-01 -1.26932492e-01  1.51325066e+00  7.81516255e-01\n",
      "  -7.13978837e-01  8.52579984e-02  7.97460186e-01 -1.86307325e+00\n",
      "  -7.57104008e-01 -3.77924984e-01  2.35778309e-01 -1.93791428e+00\n",
      "   8.92273654e-01  7.31110437e-01 -5.88619239e-01  9.08271485e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a559339ec8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANj0lEQVR4nO3db6hc9Z3H8c9ns20UrRKTqxtM8HaLDzYIm5ZrUNSiFEVjIAloaUBxQZo+SKCFIivuA8VHUdZKH0jhdo1NpVrFPyTEuNaEQixq9BpTExN2deWuTY3mBkVTonaj331wT8pNeuc3NzNn5kz6fb/gMjPnO+eeb4Z87pk5vzPn54gQgL99f9d0AwD6g7ADSRB2IAnCDiRB2IEk/r6fG5s3b14MDw/3c5NAKuPj4zp06JCnq3UVdtvXSvqppFmS/iMi1pWePzw8rLGxsW42CaBgZGSkZa3jt/G2Z0l6QNJ1khZJWmV7Uae/D0BvdfOZfYmktyPinYj4s6RfS1peT1sA6tZN2M+X9Icpj/dXy45je7XtMdtjExMTXWwOQDe6Cft0BwH+6tzbiBiNiJGIGBkaGupicwC60U3Y90taOOXxAknvddcOgF7pJuyvSrrQ9tdtf1XS9yRtqqctAHXreOgtIo7aXivpOU0Ova2PiDdr6wxArboaZ4+ILZK21NQLgB7idFkgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEl1N2Wx7XNJhSV9IOhoRI3U0BaB+XYW9clVEHKrh9wDoId7GA0l0G/aQ9Bvbr9lePd0TbK+2PWZ7bGJiosvNAehUt2G/LCK+Jek6SWtsf/vEJ0TEaESMRMTI0NBQl5sD0Kmuwh4R71W3ByU9LWlJHU0BqF/HYbd9hu2vHbsv6RpJe+pqDEC9ujkaf56kp20f+z2PRMR/1tIVgNp1HPaIeEfSP9fYC4AeYugNSIKwA0kQdiAJwg4kQdiBJOr4Igza2Lx5c7G+ffv2Yn18fLxYf+KJJ062pb9od1bj1q1bi/UjR44U6xdccEHH2541a1axjpPDnh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcfYY++uijlrWnnnqquO79999frO/bt6+jno6ZO3duy9qCBQuK6x49erRYX7x4cUc9zcQNN9xQrD/22GM923ZG7NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2WfogQceaFm78847i+tGRLF+1llnFeurVq0q1tesWdOydtFFFxXX/fzzz4v1dt+1Hx0dLdZL5yC0+x5+u+/xDw8PF+s4Hnt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfbKSy+9VKyvW7eu49992mmnFesbNmwo1pcvX97xttuZPXt2sX711VcX66XrwkvSCy+80LI2MTFRXPfuu+8u1tevX1+s43ht9+y219s+aHvPlGXn2H7e9lvV7ZzetgmgWzN5G/8LSdeesOx2Sdsi4kJJ26rHAAZY27BHxHZJH56weLmkY+89N0haUXNfAGrW6QG68yLigCRVt+e2eqLt1bbHbI+1+4wGoHd6fjQ+IkYjYiQiRtpN5AegdzoN+we250tSdXuwvpYA9EKnYd8k6Zbq/i2SNtbTDoBeaTvObvtRSVdKmmd7v6Q7Ja2T9LjtWyW9K+nGXjbZD4cPHy7WP/30045/9+23lwcrejmO3mvt/m3dHKd5+OGHi/WlS5cW6+2uS59N27BHRKsrJ3yn5l4A9BCnywJJEHYgCcIOJEHYgSQIO5AEX3GtXHPNNcX6bbfd1rJ23333Fde99957i/WrrrqqWL/iiiuK9Sa1u8z1yy+/3LL2/vvvF9dtN530Z599VqzjeOzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtlnqHQp6Weffba47u7du4v1FSvKl/B76KGHivXS5Z5PP/304rrduvHG8rebX3/99Za1e+65p+52UMCeHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJy9Bhs3li+b3+776u+++26xvnLlymL90ksvbVk788wzi+tGRLF+7rktZ/aSJN18883F+o4dO4p19A97diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2GgwPDxfrL774YrG+d+/eYr3ddee3bt1arJe0G2e3Xaw/8sgjHW+7nXnz5hXr119/fc+2/beo7Z7d9nrbB23vmbLsLtt/tL2r+ilPlA2gcTN5G/8LSddOs/z+iFhc/Wypty0AdWsb9ojYLunDPvQCoIe6OUC31vYb1dv8Oa2eZHu17THbYxMTE11sDkA3Og37zyR9Q9JiSQcktZzZMCJGI2IkIkaGhoY63ByAbnUU9oj4ICK+iIgvJf1c0pJ62wJQt47Cbnv+lIcrJe1p9VwAg6HtOLvtRyVdKWme7f2S7pR0pe3FkkLSuKQf9LDHU978+fO7ql9++eXF+scff9yy9sorrxTXvfjii4v1duPsjz/+eLH+3HPPtaxt2VIexJk1a1axPmdOy0NFmEbbsEfEqmkWP9iDXgD0EKfLAkkQdiAJwg4kQdiBJAg7kARfcT0FzJ49u1gvXe552bJldbdznLVr1xbrZ599dsvaM888U3c7KGDPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM6OxrT7+izqxZ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJtw257oe3f2t5n+03bP6yWn2P7edtvVbdMlg0MsJns2Y9K+nFE/JOkSyStsb1I0u2StkXEhZK2VY8BDKi2YY+IAxGxs7p/WNI+SedLWi5pQ/W0DZJW9KpJAN07qc/stoclfVPSDknnRcQBafIPgqRpJxyzvdr2mO2xiYmJ7roF0LEZh932mZKelPSjiPhkputFxGhEjETEyNDQUCc9AqjBjMJu+yuaDPqvIuKpavEHtudX9fmSDvamRQB1aHspaU9e7/dBSfsi4idTSpsk3SJpXXW7sScdIq0jR44U63v37i3WFy1aVGc7p7yZXDf+Mkk3S9pte1e17A5Nhvxx27dKelfSjb1pEUAd2oY9In4nqdXV/L9TbzsAeoUz6IAkCDuQBGEHkiDsQBKEHUiCKZvRmIgo1j/5pHyi5s6dO4t1xtmPx54dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnB2NmbxUAvqFPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4O05ZTz75ZLF+00039amTUwN7diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IYibzsy+U9EtJ/yDpS0mjEfFT23dJ+r6kieqpd0TEll41ilPTsmXLWtbmzp1bXPfQoUPF+vj4eCctpTWTk2qOSvpxROy0/TVJr9l+vqrdHxH/3rv2ANRlJvOzH5B0oLp/2PY+Sef3ujEA9Tqpz+y2hyV9U9KOatFa22/YXm97Tot1Vtsesz02MTEx3VMA9MGMw277TElPSvpRRHwi6WeSviFpsSb3/PdNt15EjEbESESMDA0N1dAygE7MKOy2v6LJoP8qIp6SpIj4ICK+iIgvJf1c0pLetQmgW23D7slLgD4oaV9E/GTK8vlTnrZS0p762wNQl5kcjb9M0s2SdtveVS27Q9Iq24slhaRxST/oSYc4pc2ZM+2hHEnSJZdcUlx38+bNxfqSJbyZPBkzORr/O0nTXeCbMXXgFMIZdEAShB1IgrADSRB2IAnCDiRB2IEkuJQ0GrNp06amW0iFPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJOGI6N/G7AlJ/ztl0TxJ5esFN2dQexvUviR661SdvV0QEdNe/62vYf+rjdtjETHSWAMFg9rboPYl0Vun+tUbb+OBJAg7kETTYR9tePslg9rboPYl0Vun+tJbo5/ZAfRP03t2AH1C2IEkGgm77Wtt/5ftt23f3kQPrdget73b9i7bYw33st72Qdt7piw7x/bztt+qbltfmL3/vd1l+4/Va7fL9tKGelto+7e299l+0/YPq+WNvnaFvvryuvX9M7vtWZL+W9LVkvZLelXSqojY29dGWrA9LmkkIho/AcP2tyX9SdIvI+Kiatm9kj6MiHXVH8o5EfGvA9LbXZL+1PQ03tVsRfOnTjMuaYWkf1GDr12hr++qD69bE3v2JZLejoh3IuLPkn4taXkDfQy8iNgu6cMTFi+XtKG6v0GT/1n6rkVvAyEiDkTEzur+YUnHphlv9LUr9NUXTYT9fEl/mPJ4vwZrvveQ9Bvbr9le3XQz0zgvIg5Ik/95JJ3bcD8najuNdz+dMM34wLx2nUx/3q0mwj7dVFKDNP53WUR8S9J1ktZUb1cxMzOaxrtfpplmfCB0Ov15t5oI+35JC6c8XiDpvQb6mFZEvFfdHpT0tAZvKuoPjs2gW90ebLifvxikabynm2ZcA/DaNTn9eRNhf1XShba/bvurkr4naSAuM2r7jOrAiWyfIekaDd5U1Jsk3VLdv0XSxgZ7Oc6gTOPdappxNfzaNT79eUT0/UfSUk0ekf8fSf/WRA8t+vpHSb+vft5sujdJj2rybd3/afId0a2S5kraJumt6vacAertYUm7Jb2hyWDNb6i3yzX50fANSbuqn6VNv3aFvvryunG6LJAEZ9ABSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBL/D5CHD21WFD/xAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Two helper functions\n",
    "\n",
    "# Sample minibatch of m MNIST samples from seed:\n",
    "def get_MNIST_samples(n_samples):\n",
    "    # Pick new image samples from dataset\n",
    "    # Pick random images to help with gradient descent\n",
    "    image_samples = np.zeros((n_samples, 28, 28, 1))\n",
    "\n",
    "    for i in range(0, n_samples):\n",
    "        index = random.randint(0, 59999)\n",
    "        image_samples[i] = train_images[index]\n",
    "\n",
    "    # Generate actual class labels (1)\n",
    "    y_MNIST = np.ones((n_samples, 1))\n",
    "\n",
    "    return image_samples, y_MNIST\n",
    "\n",
    "\n",
    "# Sample minibatch of m noise samples from seed:\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    latent_points = np.random.randn(latent_dim * n_samples)\n",
    "\n",
    "    # reshape into a batch of inputs for the network\n",
    "    latent_points = latent_points.reshape(n_samples, latent_dim)\n",
    "\n",
    "    return latent_points\n",
    "\n",
    "\n",
    "# Examples\n",
    "latent_points_example = generate_latent_points(100, 1)\n",
    "print(latent_points_example)\n",
    "\n",
    "# Examples\n",
    "MNIST_example, y_example = get_MNIST_samples(1)\n",
    "plt.imshow(MNIST_example[0, :, :, 0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Output:  [[0.51577157]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define all 3 models with the help of Keras\n",
    "\n",
    "# Define the standalone generator model\n",
    "def build_generator(LATENT_DIM=100):\n",
    "    model = Sequential()\n",
    "\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, input_dim=LATENT_DIM))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "\n",
    "    # Upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7, 7), activation='sigmoid', padding='same'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the standalone discriminator model\n",
    "def build_discriminator(in_shape=(28, 28, 1)):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the combined generator and discriminator model, for updating the generator\n",
    "def build_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "\n",
    "    # compile model\n",
    "    optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Example Generator\n",
    "generator_example = build_generator()\n",
    "latent_points_example = generate_latent_points(100, 1)\n",
    "\n",
    "generated_images_example = generator_example.predict(latent_points_example)\n",
    "plt.imshow(generated_images_example[0, :, :, 0], cmap='gray_r')\n",
    "\n",
    "# Example Discriminator\n",
    "discriminator_example = build_discriminator()\n",
    "\n",
    "prediction_example = discriminator_example.predict(generated_images_example)\n",
    "print('Discriminator Output: ', prediction_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(generator, discriminator, gan_model, LATENT_DIM, n_epochs=100, n_batch=256):\n",
    "    \n",
    "    batches_per_epoch = int(60000 / n_batch)\n",
    "    n_half_batch = int(n_batch / 2)\n",
    "\n",
    "    for epoch in range(0, n_epochs):\n",
    "        for batch in range(0, batches_per_epoch):\n",
    "            \n",
    "            # Get real image samples from MNIST dataset\n",
    "            image_samples, y = get_MNIST_samples(n_half_batch)\n",
    "            # Generate points from the latent space (Size: n_half_batch)\n",
    "            latent_points = generate_latent_points(LATENT_DIM, n_half_batch)\n",
    "            # Generate 'fake' class label (0)\n",
    "            y_generator = np.zeros((n_half_batch, 1))\n",
    "\n",
    "            # Generate fake images by inputting the latent_points into the generator\n",
    "            generated_images = generator.predict(latent_points)\n",
    "\n",
    "            # Step 1 ************\n",
    "            # np.vstack stacks arrays in sequence vertically (row wise) -> combine real and generated datasets\n",
    "            images_all, y_all = np.vstack((image_samples, generated_images)), np.vstack((y, y_generator))\n",
    "            # Update the discriminator loss\n",
    "            discriminator_loss, _ = discriminator.train_on_batch(images_all, y_all)\n",
    "\n",
    "            # Step 2 ************\n",
    "            # Generate points from the latent space (Size: this time n_batch)\n",
    "            latent_points = generate_latent_points(LATENT_DIM, n_batch)\n",
    "            # Generate 'fake' class label, but flip them to trip disciminator into thinking it is real (not 0, but 1)\n",
    "            y_generator_flipped = np.ones((n_batch, 1))\n",
    "            generator_loss = gan_model.train_on_batch(latent_points, y_generator_flipped)\n",
    "\n",
    "            print('Epoch: ', epoch, '   Batch Number: ', batch, ' / ', batches_per_epoch, '   Generator Loss: ', generator_loss, '   Discriminator Loss: ', discriminator_loss)\n",
    "\n",
    "            if batch % 25 == 0:\n",
    "                # Matplotlib to capture generated the images ###\n",
    "                fig = plt.figure()\n",
    "                for a in range(32):\n",
    "                    ax = fig.add_subplot(6, 6, a + 1)\n",
    "                    plt.imshow(generated_images[a, :, :, 0], cmap='gray_r')\n",
    "\n",
    "                # Plots are saved with 4 digit numbers in the name to avoid flickering in the video\n",
    "                plt.savefig('plot  epoch ' + '%02d' % epoch + '  batch ' + '%05d' % batch + '.png', dpi=600)\n",
    "                plt.close()\n",
    "                # End Matplotlib                             ###\n",
    "\n",
    "                # Save the generator and discriminator\n",
    "                generator.save('generator.h5')\n",
    "                discriminator.save('discriminator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Batch Number:  0  /  234    Generator Loss:  0.719807505607605    Discriminator Loss:  0.6784718036651611\n",
      "Epoch:  0    Batch Number:  1  /  234    Generator Loss:  0.7372521162033081    Discriminator Loss:  0.6744446754455566\n",
      "Epoch:  0    Batch Number:  2  /  234    Generator Loss:  0.7525892853736877    Discriminator Loss:  0.6688157320022583\n",
      "Epoch:  0    Batch Number:  3  /  234    Generator Loss:  0.7734922170639038    Discriminator Loss:  0.6581707000732422\n",
      "Epoch:  0    Batch Number:  4  /  234    Generator Loss:  0.782932698726654    Discriminator Loss:  0.6543998718261719\n",
      "Epoch:  0    Batch Number:  5  /  234    Generator Loss:  0.7962650656700134    Discriminator Loss:  0.6462928652763367\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b27f18687b17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLATENT_DIM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-a67c4b13da31>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(generator, discriminator, gan_model, LATENT_DIM, n_epochs, n_batch)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# Generate 'fake' class label, but flip them to trip disciminator into thinking it is real (not 0, but 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0my_generator_flipped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mgenerator_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_points\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_generator_flipped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'   Batch Number: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' / '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatches_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'   Generator Loss: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'   Discriminator Loss: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1346\u001b[0m                                                     class_weight)\n\u001b[0;32m   1347\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\Notebook\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build the GAN ************\n",
    "\n",
    "# Size of the latent space\n",
    "LATENT_DIM = 100\n",
    "\n",
    "\n",
    "# Define the discriminator\n",
    "discriminator = build_discriminator()\n",
    "# Define the generator\n",
    "generator = build_generator(LATENT_DIM)\n",
    "# Define the gan\n",
    "gan_model = build_gan(generator, discriminator)\n",
    "\n",
    "\n",
    "# Train model\n",
    "train(generator, discriminator, gan_model, LATENT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Notebook",
   "language": "python",
   "name": "notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
